{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division      #除数可以显示为float\n",
    "\n",
    "from six import StringIO    #使用聚宽readfile函数\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time                 #使用time stamp\n",
    "import datetime             \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "import talib\n",
    "\n",
    "from QuantLib import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "QuantLib.py:196: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  /(obv_price['high'].values[i]-obv_price['low'].values[i])\n"
     ]
    }
   ],
   "source": [
    "# 从基础数据表中读取数据，生成指标数据\n",
    "data_test = pd.DataFrame()\n",
    "data_test,Col_Name = Indicator_Generate(pd.read_csv(\"raw_data_price2.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 63)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 扩展数据，生成多列数据\n",
    "data_extend_test = pd.DataFrame()\n",
    "data_extend_test,Col_Total = Indicator_Extend(data_test,3,Col_Name)\n",
    "data_extend_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52858, 63)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 删除无效数据，更改“平”为“负”\n",
    "data_delete = delete_data(data_extend_test)\n",
    "data_delete.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据归一化，并且生成均方差列表\n",
    "data_normalize = pd.DataFrame()\n",
    "MS_pd = pd.DataFrame()\n",
    "data_normalize,MS_pd = normalize_into_pd(data_delete,Col_Total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提示：这里需要对均方差列表进行手工操作，以便之后进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始进行量化预测\n",
    "from __future__ import division      #除数可以显示为float\n",
    "from six import StringIO    #使用聚宽readfile函数\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time                 #使用time stamp\n",
    "import datetime             \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "\n",
    "# 最基本的KNN算法\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 导入样本拆分模块\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 导入KNN半径算法\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "\n",
    "# 评分函数\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# 交叉评分函数\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n",
      "2018-07-17\n",
      "2018-07-16\n",
      "2018-07-13\n",
      "2018-07-12\n",
      "2018-07-11\n"
     ]
    }
   ],
   "source": [
    "Stock_list = set(list(data_delete['code']))\n",
    "score_stock_dict = {}\n",
    "k = 11\n",
    "\n",
    "win_num = 0\n",
    "try_num = 0\n",
    "for stock_name in Stock_list:\n",
    "\n",
    "    # 测试数据\n",
    "    X =  np.array(data_delete.loc[data_delete['code'] == stock_name,Col_Total])\n",
    "    # 测试结果\n",
    "    y = np.array(data_delete.loc[data_delete['code'] == stock_name,\"win_rate\"])\n",
    "\n",
    "    # 分别生成训练数据、测试数据\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0)\n",
    "    \n",
    "    # 单一模型预测\n",
    "    # 训练模型\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    # 读取删除的数据作为盲测结果：[-10:-5]\n",
    "    \n",
    "    for i in range(1,6):\n",
    "        predict_data_series = pd.Series(data_extend_test.loc[data_extend_test['code'] == stock_name,Col_Total].iloc[int(-5-i)])\n",
    "        print data_extend_test.loc[data_extend_test['code'] == stock_name,'date'].iloc[int(-5-i)]\n",
    "        predict_data_normal_pd = normalize_by_list(MS_pd,predict_data_series,stock_name,Col_Total)\n",
    "        y_pred = model.predict([np.array(predict_data_normal_pd.iloc[0,:])])\n",
    "        if (y_pred[0] == 1):\n",
    "            try_num += 1 \n",
    " \n",
    "        \n",
    "        if (y_pred[0] == 1)&(data_extend_test.loc[data_extend_test['code'] == stock_name,'win_rate'].iloc[int(-5-i)] == 1):\n",
    "            win_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_stock_pd = pd.DataFrame.from_dict(score_stock_dict)\n",
    "score_stock_pd.to_csv(\"stock_rate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "50\n",
      "set(['2018-07-13', '2018-07-12', '2018-07-11', '2018-07-17', '2018-07-16'])\n"
     ]
    }
   ],
   "source": [
    "print win_num\n",
    "print try_num\n",
    "print set([\"2018-07-11\",\"2018-07-12\",\"2018-07-13\",\"2018-07-16\",\"2018-07-17\"])-set(list(data_delete.loc[:,\"date\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始针对数据进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000568.XSHE\n",
      "2018-06-27\n",
      "000100.XSHE\n",
      "2018-06-27\n",
      "000630.XSHE\n",
      "2018-06-27\n",
      "000559.XSHE\n",
      "2018-06-27\n",
      "000413.XSHE\n",
      "2018-06-27\n",
      "000728.XSHE\n",
      "2018-06-27\n",
      "000002.XSHE\n",
      "2018-06-27\n",
      "000540.XSHE\n",
      "2018-06-27\n",
      "000402.XSHE\n",
      "2018-06-27\n",
      "000060.XSHE\n",
      "2018-06-27\n",
      "000538.XSHE\n",
      "2018-06-27\n",
      "000425.XSHE\n",
      "2018-06-27\n",
      "000625.XSHE\n",
      "2018-06-27\n",
      "000069.XSHE\n",
      "2018-06-27\n",
      "000725.XSHE\n",
      "2018-06-27\n",
      "000415.XSHE\n",
      "2018-06-27\n",
      "000423.XSHE\n",
      "2018-06-27\n",
      "000503.XSHE\n",
      "2018-06-27\n",
      "000623.XSHE\n",
      "2018-06-27\n",
      "000001.XSHE\n",
      "2018-06-27\n",
      "000333.XSHE\n",
      "2018-06-27\n",
      "000627.XSHE\n",
      "2018-06-27\n",
      "000651.XSHE\n",
      "2018-06-27\n",
      "000063.XSHE\n",
      "2018-06-27\n",
      "000709.XSHE\n",
      "2018-06-27\n",
      "000166.XSHE\n",
      "2018-06-27\n",
      "000723.XSHE\n",
      "2018-06-27\n",
      "000338.XSHE\n",
      "2018-06-27\n",
      "000671.XSHE\n",
      "2018-06-27\n",
      "000157.XSHE\n",
      "2018-06-27\n"
     ]
    }
   ],
   "source": [
    "Stock_list = set(list(data_normalize['code']))\n",
    "Buy_List_Dict = {}\n",
    "k = 11\n",
    "MS_pd = pd.read_csv(\"MS_list.csv\")\n",
    "\n",
    "\n",
    "for stock_name in Stock_list:\n",
    "\n",
    "    # 测试数据\n",
    "    X =  np.array(data_normalize.loc[data_normalize['code'] == stock_name,Col_Total])\n",
    "    # 测试结果\n",
    "    y = np.array(data_normalize.loc[data_normalize['code'] == stock_name,\"win_rate\"])\n",
    "\n",
    "    # 分别生成训练数据、测试数据\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X,y,test_size = 0)\n",
    "    \n",
    "    # 单一模型预测\n",
    "    # 训练模型\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    predict_data_series = pd.Series(data_extend_test.loc[data_extend_test['code'] == stock_name,Col_Total].iloc[-1,:])\n",
    "    \n",
    "    \n",
    "    predict_data_normal_pd = normalize_by_list(MS_pd,predict_data_series,stock_name,Col_Total)\n",
    "    \n",
    "    y_pred = model.predict([list(predict_data_normal_pd.iloc[0,:])])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    print stock_name \n",
    "    print str(data_extend_test.loc[data_extend_test['code'] == stock_name,'date'][-20:-19].values[0])\n",
    "    predict_data_normal_pd.to_csv(stock_name+str(data_extend_test.loc[data_extend_test['code'] == stock_name,'date'][-20:-19].values[0])+\".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 13] Permission denied: 'verify.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-9adeb5387e39>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_normalize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"verify.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   1743\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[1;32m-> 1745\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pandas\\io\\formats\\csvs.pyc\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[0;32m    155\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m                                      compression=self.compression)\n\u001b[0m\u001b[0;32m    157\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\pandas\\io\\common.pyc\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    395\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPY2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;31m# Python 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;31m# Python 3 and encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 13] Permission denied: 'verify.csv'"
     ]
    }
   ],
   "source": [
    "data_normalize.to_csv(\"verify.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取模块进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进入聚宽进行验证"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
